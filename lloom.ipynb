{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import time\n",
    "import text_lloom.workbench as wb\n",
    "from text_lloom.llm import Model, EmbedModel\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# --- Gemini API Key ---\n",
    "print(\"üîç Step 1: Loading API Key...\")\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"‚ùå Error: GOOGLE_API_KEY not found in .env\")\n",
    "    sys.exit(1)\n",
    "print(\"‚úÖ API Key loaded successfully\")\n",
    "\n",
    "# --- Model Setup ---\n",
    "\n",
    "print(\"\\nüîç Step 2: Setting up model functions...\")\n",
    "# SETUP functions\n",
    "def setup_llm_fn(api_key):\n",
    "    print(\"   ‚öôÔ∏è  Setting up LLM...\")\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    print(\"   ‚úÖ LLM setup complete\")\n",
    "    return client\n",
    "\n",
    "def setup_embed_fn(api_key):\n",
    "    print(\"   ‚öôÔ∏è  Setting up Embedding model...\")\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    print(\"   ‚úÖ Embedding model setup complete\")\n",
    "    return client\n",
    "\n",
    "print(\"‚úÖ Model functions ready\")\n",
    "\n",
    "# CALL functions\n",
    "print(\"\\nüîç Step 3: Setting up call functions...\")\n",
    "async def call_llm_fn(model, prompt):\n",
    "    if \"system_prompt\" not in model.args:\n",
    "        model.args[\"system_prompt\"] = \"You are a helpful assistant who helps with identifying patterns in text examples.\"\n",
    "    if \"temperature\" not in model.args:\n",
    "        model.args[\"temperature\"] = 0\n",
    "    \n",
    "    try:\n",
    "        # Check if json is requested in prompt to decide on response_mime_type\n",
    "        config = {\n",
    "            \"temperature\": model.args[\"temperature\"],\n",
    "            \"max_output_tokens\": 8192,\n",
    "        }\n",
    "        \n",
    "        # Only enforce JSON if it looks like the prompt expects it\n",
    "        if \"JSON\" in prompt or \"json\" in prompt:\n",
    "             config[\"response_mime_type\"] = \"application/json\"\n",
    "\n",
    "        res = model.client.models.generate_content(\n",
    "            model=model.name,\n",
    "            contents=prompt,\n",
    "            config=config\n",
    "        )\n",
    "        res_parsed = res.text if res and hasattr(res, 'text') else None\n",
    "        tokens = [0, 0]\n",
    "        return res_parsed, tokens\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå LLM Error: {str(e)}\")\n",
    "        return None, [0, 0]\n",
    "\n",
    "def call_embed_fn(model, text_arr):\n",
    "    print(f\"   üîó Embedding call initiated with {len(text_arr)} items...\")\n",
    "    \n",
    "    # Ensure text_arr is a list of strings\n",
    "    if isinstance(text_arr, str):\n",
    "        text_arr = [text_arr]\n",
    "    \n",
    "    # Filter out empty strings which cause API errors\n",
    "    valid_indices = [i for i, t in enumerate(text_arr) if t and isinstance(t, str) and t.strip()]\n",
    "    \n",
    "    if not valid_indices:\n",
    "        print(\"   ‚ö†Ô∏è Warning: No valid text to embed.\")\n",
    "        return [[0.0] * 3072] * len(text_arr), [0, 0]  # CHANGED: 3072 dimensions\n",
    "    \n",
    "    filtered_text = [text_arr[i] for i in valid_indices]\n",
    "    embeddings_map = {} \n",
    "    \n",
    "    # Process in small batches\n",
    "    batch_size = 10\n",
    "    \n",
    "    for i in range(0, len(filtered_text), batch_size):\n",
    "        batch = filtered_text[i:i+batch_size]\n",
    "        start_idx = i\n",
    "        \n",
    "        max_retries = 3\n",
    "        batch_embeddings = []\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # CHANGED: Using gemini-embedding-001 (the working model)\n",
    "                res = model.client.models.embed_content(\n",
    "                    model=\"gemini-embedding-001\",\n",
    "                    contents=batch,\n",
    "                )\n",
    "                \n",
    "                # Extract embeddings - response has 'embeddings' attribute\n",
    "                if hasattr(res, 'embeddings') and res.embeddings:\n",
    "                    batch_embeddings = [e.values for e in res.embeddings]\n",
    "                \n",
    "                if batch_embeddings:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                sleep_time = 2 * (attempt + 1)\n",
    "                if \"429\" in str(e):\n",
    "                    print(f\"   ‚ö†Ô∏è Rate limited. Waiting {sleep_time}s...\")\n",
    "                elif \"500\" in str(e) or \"503\" in str(e):\n",
    "                    print(f\"   ‚ö†Ô∏è Server error. Waiting {sleep_time}s...\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå Embedding Error: {str(e)}\")\n",
    "                    break\n",
    "                time.sleep(sleep_time)\n",
    "        \n",
    "        # Fill in embeddings for this batch\n",
    "        if batch_embeddings:\n",
    "             for j, emb in enumerate(batch_embeddings):\n",
    "                 if j < len(batch): \n",
    "                    original_idx = valid_indices[start_idx + j]\n",
    "                    embeddings_map[original_idx] = emb\n",
    "        else:\n",
    "            print(f\"   ‚ùå Batch failed (size {len(batch)}). API returned no embeddings.\")\n",
    "            raise RuntimeError(f\"Embedding failed for batch starting at {start_idx}\")\n",
    "\n",
    "    # reconstruct full list in original order\n",
    "    result_embeddings = []\n",
    "    for i in range(len(text_arr)):\n",
    "        if i in embeddings_map:\n",
    "            result_embeddings.append(embeddings_map[i])\n",
    "        else:\n",
    "            result_embeddings.append([0.0] * 3072)  # CHANGED: 3072 dimensions\n",
    "\n",
    "    tokens = [0, 0]\n",
    "    print(f\"   ‚úÖ Embedding complete. {len(result_embeddings)} vectors.\")\n",
    "    return result_embeddings, tokens\n",
    "\n",
    "print(\"‚úÖ Call functions ready\")\n",
    "\n",
    "# --- LLooM Instance ---\n",
    "\n",
    "print(\"\\nüîç Step 4: Loading CSV file...\")\n",
    "df = pd.read_csv(\"events.csv\")\n",
    "print(f\"‚úÖ Loaded {len(df)} events\")\n",
    "\n",
    "# Ensure we have enough data (duplicate if too small for testing UMAP)\n",
    "if len(df) < 15:\n",
    "    print(\"‚ö†Ô∏è Warning: Dataset is very small. Duplicating data for UMAP stability...\")\n",
    "    multiplier = (20 // len(df)) + 1\n",
    "    df = pd.concat([df] * multiplier, ignore_index=True)\n",
    "    print(f\"   New shape: {df.shape}\")\n",
    "\n",
    "print(\"\\nüîç Step 5: Creating LLooM instance with Gemini models...\")\n",
    "# Create the LLooM instance with custom Gemini models\n",
    "l = wb.lloom(\n",
    "    df=df,\n",
    "    text_col=\"event\",\n",
    "    \n",
    "    # Custom Gemini models\n",
    "    distill_model=Model(\n",
    "        setup_fn=setup_llm_fn,\n",
    "        fn=call_llm_fn,\n",
    "        name=\"gemini-2.5-flash\",\n",
    "        cost=[0.0005/1000, 0.0015/1000], \n",
    "        rate_limit=(60, 60), \n",
    "        context_window=32000, \n",
    "        api_key=api_key\n",
    "    ),\n",
    "    cluster_model=EmbedModel(\n",
    "        setup_fn=setup_embed_fn,\n",
    "        fn=call_embed_fn,\n",
    "        name=\"gemini-embedding-001\",  # CHANGED: from \"models/embedding-001\"\n",
    "        cost=(0.00001/1000), \n",
    "        batch_size=10, \n",
    "        api_key=api_key\n",
    "    ),\n",
    "    synth_model=Model(\n",
    "        setup_fn=setup_llm_fn,\n",
    "        fn=call_llm_fn,\n",
    "        name=\"gemini-2.5-flash\", \n",
    "        cost=[0.01/1000, 0.03/1000], \n",
    "        rate_limit=(60,60), \n",
    "        context_window=32000, \n",
    "        api_key=api_key\n",
    "    ),\n",
    "    score_model=Model(\n",
    "        setup_fn=setup_llm_fn,\n",
    "        fn=call_llm_fn,\n",
    "        name=\"gemini-2.5-flash\", \n",
    "        cost=[0.0005/1000, 0.0015/1000], \n",
    "        rate_limit=(60,60),\n",
    "        context_window=32000,\n",
    "        api_key=api_key\n",
    "        ),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLooM instance created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d501ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ Running LLooM Auto Generation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Generating features with auto parameters...\")\n",
    "print(\"   (This may take several minutes...)\\n\")\n",
    "\n",
    "# Run gen_auto without interactive prompt by using debug=False\n",
    "score_df = await l.gen_auto(\n",
    "    max_concepts=5,\n",
    "    debug=False  # Skip interactive prompt\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Generation and scoring complete!\")\n",
    "print(f\"   Score DataFrame shape: {score_df.shape if score_df is not None else 'None'}\")\n",
    "\n",
    "# Display results\n",
    "if score_df is not None:\n",
    "    print(\"\\nüìã Score Results Preview:\")\n",
    "    print(score_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ee11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc906660",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.vis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b728d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
